{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beauty LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edenlau/Text-Classification/blob/master/Beauty_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3u5gbePAKPs",
        "colab_type": "code",
        "outputId": "b31cc352-a7e9-4b03-c8e9-71200729cae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyU2QX7ApUn",
        "colab_type": "code",
        "outputId": "87aa5861-9583-410e-b5ca-2285d43d6a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /gdrive/My Drive/LSTM\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import font_manager\n",
        "from itertools import accumulate\n",
        "\n",
        "# è®¾ç½®matplotlibç»˜å›¾æ—¶çš„å­—ä½“\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "my_font = font_manager.FontProperties(fname=\"./NotoSansSC-Regular.otf\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH03vtvo_zil",
        "colab_type": "code",
        "outputId": "aa1130bd-62c5-4d1c-ae4b-739851d32bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# ç»Ÿè®¡å¥å­é•¿åº¦åŠé•¿åº¦å‡ºç°çš„é¢‘æ•°\n",
        "filepath = \"./beauty_corpus.csv\"\n",
        "df = pd.read_csv(filepath)\n",
        "print(df.groupby('label')['label'].count())\n",
        "\n",
        "df['length'] = df['evaluation'].apply(lambda x: len(x))\n",
        "len_df = df.groupby('length').count()\n",
        "sent_length = len_df.index.tolist()\n",
        "sent_freq = len_df['evaluation'].tolist()\n",
        "\n",
        "# ç»˜åˆ¶å¥å­é•¿åº¦åŠå‡ºç°é¢‘æ•°ç»Ÿè®¡å›¾\n",
        "plt.bar(sent_length, sent_freq)\n",
        "plt.title(\"å¥å­é•¿åº¦åŠå‡ºç°é¢‘æ•°ç»Ÿè®¡å›¾\", fontproperties=my_font)\n",
        "plt.xlabel(\"å¥å­é•¿åº¦\", fontproperties=my_font)\n",
        "plt.ylabel(\"å¥å­é•¿åº¦å‡ºç°çš„é¢‘æ•°\", fontproperties=my_font)\n",
        "plt.savefig(\"å¥å­é•¿åº¦åŠå‡ºç°é¢‘æ•°ç»Ÿè®¡å›¾.png\")\n",
        "#files.download(\"å¥å­é•¿åº¦åŠå‡ºç°é¢‘æ•°ç»Ÿè®¡å›¾.png\")\n",
        "plt.close()\n",
        "\n",
        "# ç»˜åˆ¶å¥å­é•¿åº¦ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF)\n",
        "sent_pentage_list = [(count/sum(sent_freq)) for count in accumulate(sent_freq)]\n",
        "\n",
        "# ç»˜åˆ¶CDF\n",
        "plt.plot(sent_length, sent_pentage_list)\n",
        "\n",
        "# å¯»æ‰¾åˆ†ä½ç‚¹ä¸ºquantileçš„å¥å­é•¿åº¦\n",
        "quantile = 0.91\n",
        "#print(list(sent_pentage_list))\n",
        "for length, per in zip(sent_length, sent_pentage_list):\n",
        "    if round(per, 2) == quantile:\n",
        "        index = length\n",
        "        break\n",
        "print(\"\\nåˆ†ä½ç‚¹ä¸º%sçš„å¥å­é•¿åº¦:%d.\" % (quantile, index))\n",
        "\n",
        "# ç»˜åˆ¶å¥å­é•¿åº¦ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾\n",
        "plt.plot(sent_length, sent_pentage_list)\n",
        "plt.hlines(quantile, 0, index, colors=\"c\", linestyles=\"dashed\")\n",
        "plt.vlines(index, 0, quantile, colors=\"c\", linestyles=\"dashed\")\n",
        "plt.text(0, quantile, str(quantile))\n",
        "plt.text(index, 0, str(index))\n",
        "plt.title(\"å¥å­é•¿åº¦ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾\", fontproperties=my_font)\n",
        "plt.xlabel(\"å¥å­é•¿åº¦\", fontproperties=my_font)\n",
        "plt.ylabel(\"å¥å­é•¿åº¦ç´¯ç§¯é¢‘ç‡\", fontproperties=my_font)\n",
        "plt.savefig(\"å¥å­é•¿åº¦ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾.png\")\n",
        "#files.download(\"å¥å­é•¿åº¦ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾.png\") \n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label\n",
            "ä¸­æ€§     1105\n",
            "æ­£é¢    15906\n",
            "è´Ÿé¢      823\n",
            "Name: label, dtype: int64\n",
            "\n",
            "åˆ†ä½ç‚¹ä¸º0.91çš„å¥å­é•¿åº¦:80.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYd3Bv2O1mIz",
        "colab_type": "code",
        "outputId": "bb941d70-71c9-4e0e-85f6-fb994f65a301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1058
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import np_utils, plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# å¯¼å…¥æ•°æ®\n",
        "# æ–‡ä»¶çš„æ•°æ®ä¸­ï¼Œç‰¹å¾ä¸ºevaluation, ç±»åˆ«ä¸ºlabel.\n",
        "def load_data(filepath, input_shape=20):\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # æ ‡ç­¾åŠè¯æ±‡è¡¨\n",
        "    labels, vocabulary = list(df['label'].unique()), list(df['evaluation'].unique())\n",
        "\n",
        "    # æ„é€ å­—ç¬¦çº§åˆ«çš„ç‰¹å¾\n",
        "    string = ''\n",
        "    for word in vocabulary:\n",
        "        string += word\n",
        "\n",
        "    vocabulary = set(string)\n",
        "\n",
        "    # å­—å…¸åˆ—è¡¨\n",
        "    word_dictionary = {word: i+1 for i, word in enumerate(vocabulary)}\n",
        "    with open('word_dict.pk', 'wb') as f:\n",
        "        pickle.dump(word_dictionary, f)\n",
        "    inverse_word_dictionary = {i+1: word for i, word in enumerate(vocabulary)}\n",
        "    label_dictionary = {label: i for i, label in enumerate(labels)}\n",
        "    with open('label_dict.pk', 'wb') as f:\n",
        "        pickle.dump(label_dictionary, f)\n",
        "    output_dictionary = {i: labels for i, labels in enumerate(labels)}\n",
        "\n",
        "    vocab_size = len(word_dictionary.keys()) # è¯æ±‡è¡¨å¤§å°\n",
        "    label_size = len(label_dictionary.keys()) # æ ‡ç­¾ç±»åˆ«æ•°é‡\n",
        "\n",
        "    # åºåˆ—å¡«å……ï¼ŒæŒ‰input_shapeå¡«å……ï¼Œé•¿åº¦ä¸è¶³çš„æŒ‰0è¡¥å……\n",
        "    x = [[word_dictionary[word] for word in sent] for sent in df['evaluation']]\n",
        "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
        "    y = [[label_dictionary[sent]] for sent in df['label']]\n",
        "    y = [np_utils.to_categorical(label, num_classes=label_size) for label in y]\n",
        "    y = np.array([list(_[0]) for _ in y])\n",
        "\n",
        "    return x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary\n",
        "\n",
        "# åˆ›å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œ Embedding + LSTM + Softmax.\n",
        "def create_LSTM(n_units, input_shape, output_dim, filepath):\n",
        "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size + 1, output_dim=output_dim,\n",
        "                        input_length=input_shape, mask_zero=True))\n",
        "    model.add(LSTM(n_units, input_shape=(x.shape[0], x.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(label_size, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    plot_model(model, to_file='./model_lstm.png', show_shapes=True)\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# æ¨¡å‹è®­ç»ƒ\n",
        "def model_train(input_shape, filepath, model_save_path):\n",
        "\n",
        "    # å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå æ¯”ä¸º9:1\n",
        "    # input_shape = 100\n",
        "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)\n",
        "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
        "\n",
        "    # æ¨¡å‹è¾“å…¥å‚æ•°ï¼Œéœ€è¦è‡ªå·±æ ¹æ®éœ€è¦è°ƒæ•´\n",
        "    n_units = 100\n",
        "    batch_size = 32\n",
        "    epochs = 5\n",
        "    output_dim = 20\n",
        "\n",
        "    # æ¨¡å‹è®­ç»ƒ\n",
        "    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)\n",
        "    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    # æ¨¡å‹ä¿å­˜\n",
        "    lstm_model.save(model_save_path)\n",
        "\n",
        "    N = test_x.shape[0]  # æµ‹è¯•çš„æ¡æ•°\n",
        "    predict = []\n",
        "    label = []\n",
        "    for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
        "        sentence = [inverse_word_dictionary[i] for i in test_x[start] if i != 0]\n",
        "        y_predict = lstm_model.predict(test_x[start:end])\n",
        "        label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
        "        label_true = output_dictionary[np.argmax(test_y[start:end])]\n",
        "        print(''.join(sentence), label_true, label_predict) # è¾“å‡ºé¢„æµ‹ç»“æœ\n",
        "        predict.append(label_predict)\n",
        "        label.append(label_true)\n",
        "\n",
        "    acc = accuracy_score(predict, label) # é¢„æµ‹å‡†ç¡®ç‡\n",
        "    print('æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º: %s.' % acc)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    filepath = './beauty_corpus.csv'\n",
        "    input_shape = 180\n",
        "    model_save_path = './beauty_corpus_model.h5'\n",
        "    model_train(input_shape, filepath, model_save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 03:36:52.111222 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0618 03:36:52.142178 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0618 03:36:52.151009 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0618 03:36:52.393677 140485073356672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0618 03:36:52.428653 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0618 03:36:52.437150 140485073356672 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0618 03:36:52.464080 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0618 03:36:52.486501 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 180, 20)           67060     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               48400     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 115,864\n",
            "Trainable params: 115,864\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "16051/16051 [==============================] - 188s 12ms/step - loss: 0.3340 - acc: 0.9137\n",
            "Epoch 2/5\n",
            "16051/16051 [==============================] - 179s 11ms/step - loss: 0.2078 - acc: 0.9278\n",
            "Epoch 3/5\n",
            " 2816/16051 [====>.........................] - ETA: 2:23 - loss: 0.1943 - acc: 0.9315"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c8a47d41ed40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./beauty_corpus_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-c8a47d41ed40>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(input_shape, filepath, model_save_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# æ¨¡å‹è®­ç»ƒ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# æ¨¡å‹ä¿å­˜\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl927EBSkHSV",
        "colab_type": "code",
        "outputId": "df76aa74-5ecb-4403-b137-52c299f06e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Import the necessary modules\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# å¯¼å…¥å­—å…¸\n",
        "with open('word_dict.pk', 'rb') as f:\n",
        "    word_dictionary = pickle.load(f)\n",
        "with open('label_dict.pk', 'rb') as f:\n",
        "    output_dictionary = pickle.load(f)\n",
        "\n",
        "try:\n",
        "    # æ•°æ®é¢„å¤„ç†\n",
        "    input_shape = 180\n",
        "    sent = \"ä½œä¸ºä¸€ä¸ªç”·çš„ï¼Œæˆ‘æ˜¯ä¸æ‡‚å£çº¢æ€ä¹ˆæ ·çš„ï¼Œä¹°æ¥é€äººçš„ï¼Œä½†ä½œä¸ºä¸€ä¸ªç‰Œå­çš„æ——èˆ°åº—åŒ…è£…éƒ½æ˜¯è¿™ä¹ˆç®€é™‹çš„å—\"\n",
        "    x = [[word_dictionary[word] for word in sent]]\n",
        "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
        "\n",
        "    # è½½å…¥æ¨¡å‹\n",
        "    model_save_path = './corpus_model.h5'\n",
        "    lstm_model = load_model(model_save_path)\n",
        "\n",
        "    # æ¨¡å‹é¢„æµ‹\n",
        "    y_predict = lstm_model.predict(x)\n",
        "    label_dict = {v:k for k,v in output_dictionary.items()}\n",
        "    print('è¾“å…¥è¯­å¥: %s' % sent)\n",
        "    print('æƒ…æ„Ÿé¢„æµ‹ç»“æœ: %s' % label_dict[np.argmax(y_predict)])\n",
        "\n",
        "except KeyError as err:\n",
        "    print(\"æ‚¨è¾“å…¥çš„å¥å­æœ‰æ±‰å­—ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè¯·é‡æ–°è¾“å…¥ï¼\")\n",
        "    print(\"ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯ä¸ºï¼š%s.\" % err)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "è¾“å…¥è¯­å¥: ä½œä¸ºä¸€ä¸ªç”·çš„ï¼Œæˆ‘æ˜¯ä¸æ‡‚å£çº¢æ€ä¹ˆæ ·çš„ï¼Œä¹°æ¥é€äººçš„ï¼Œä½†ä½œä¸ºä¸€ä¸ªç‰Œå­çš„æ——èˆ°åº—åŒ…è£…éƒ½æ˜¯è¿™ä¹ˆç®€é™‹çš„å—\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j3JrrTg1o0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bdf = pd.read_csv(\"./ECData.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzr9lVEB1-ME",
        "colab_type": "code",
        "outputId": "a503f00b-35d4-4776-e948-085d35ed849a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2927
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Import the necessary modules\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# å¯¼å…¥å­—å…¸\n",
        "with open('word_dict.pk', 'rb') as f:\n",
        "    word_dictionary = pickle.load(f)\n",
        "with open('label_dict.pk', 'rb') as f:\n",
        "    output_dictionary = pickle.load(f)\n",
        "\n",
        "sbdf = bdf.sample(frac=0.01) # set sampling ratio for evaluation\n",
        "    \n",
        "for i, j in sbdf.iterrows(): \n",
        "    sent = sbdf.loc[i, \"Comment\"]\n",
        "    try:\n",
        "        # æ•°æ®é¢„å¤„ç†\n",
        "        input_shape = 180\n",
        "        #sent = \"ä½œä¸ºä¸€ä¸ªç”·çš„ï¼Œæˆ‘æ˜¯ä¸æ‡‚å£çº¢æ€ä¹ˆæ ·çš„ï¼Œä¹°æ¥é€äººçš„ï¼Œä½†ä½œä¸ºä¸€ä¸ªç‰Œå­çš„æ——èˆ°åº—åŒ…è£…éƒ½æ˜¯è¿™ä¹ˆç®€é™‹çš„å—\"\n",
        "        x = [[word_dictionary[word] for word in sent]]\n",
        "        x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
        "\n",
        "        # è½½å…¥æ¨¡å‹\n",
        "        model_save_path = './corpus_model.h5'\n",
        "        lstm_model = load_model(model_save_path)\n",
        "\n",
        "        # æ¨¡å‹é¢„æµ‹\n",
        "        y_predict = lstm_model.predict(x)\n",
        "        label_dict = {v:k for k,v in output_dictionary.items()}\n",
        "        print('è¾“å…¥è¯­å¥: %s' % sent)\n",
        "        print('æƒ…æ„Ÿé¢„æµ‹ç»“æœ: %s' % label_dict[np.argmax(y_predict)])\n",
        "\n",
        "    except KeyError as err:\n",
        "        print(\"æ‚¨è¾“å…¥çš„å¥å­æœ‰æ±‰å­—ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè¯·é‡æ–°è¾“å…¥ï¼\")\n",
        "        print(\"ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯ä¸ºï¼š%s.\" % err)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "è¾“å…¥è¯­å¥: å¾ˆå¿«å°±æ”¶åˆ°ï¼Œå¾ˆå–œæ¬¢å‘³é“é¦™é¦™çš„ã€‚é¢œè‰²æŒºåƒmacçš„divaï¼Œä¸è¿‡yslçš„å”‡é‡‰æˆ‘çœŸçš„å¾ˆå–œæ¬¢ä¸å¹²è€Œä¸”ä¸æ²¾æ¯ã€‚ä¸Šå˜´æˆ‘æ˜¯æ— è®ºä»€ä¹ˆè‰²éƒ½ä¼šåç´«ï¼Œä¸€å®šæ¶‚æ‰‹ä¸Šå°±æ˜¯ä¸­é—´é‚£ä¸ªé¢œè‰²ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ç›¸å½“æ»¡æ„çš„ä¸€æ¬¡è´­ç‰©\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: å¥½çœ‹ã€‚å–œæ¬¢ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é¢œå€¼çœŸçš„è¶…é«˜ï¼ŒåŒ…è£…ä¹Ÿå¾ˆé«˜å¤§ä¸Šï¼Œå¾ˆå–œæ¬¢ï¼Œæ§æ²¹æ•ˆæœçœŸçš„ä¸é”™ï¼Œä½†éšå½¢æ¯›å­”æ„Ÿè§‰ä½œç”¨ä¸å¤§ï¼Œä¹Ÿæœ‰å¯èƒ½ä¸ªäººä¸æ€ä¹ˆä¼šç”¨ï¼Œç„¶åè¿˜æœ‰å°±å¯ä»¥å®šå¦†ä¹Ÿä¸çŸ¥é“æ€ä¹ˆç”¨ï¼Œæ€»çš„è¿˜æ˜¯æ¯”è¾ƒæ»¡æ„\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¥½çœ‹ï¼Œè¿˜æœ‰è‚¡æ‚ æ‚ çš„é¦™å‘³ï½\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: åŒ11è´­çš„è¿™ä¹ˆå¿«å°±æ”¶åˆ°äº†å¤ªç¾äº†ä¹Ÿè°¢è°¢å¿«é€’å‘˜ï¼ŒåŒ…è£…å®Œç¾ï¼Œé¢œè‰²å®Œç¾ï¼Œè¿˜æ”¶åˆ°äº†å¥½å¤šèµ å“ï¼Œéå¸¸æ„‰å¿«çš„è´­ç‰©ï¼åŒ11è´­çš„è¿™ä¹ˆå¿«å°±æ”¶åˆ°äº†å¤ªç¾äº†ä¹Ÿè°¢è°¢å¿«é€’å‘˜ï¼ŒåŒ…è£…å®Œç¾ï¼Œé¢œè‰²å®Œç¾ï¼Œè¿˜æ”¶åˆ°äº†å¥½å¤šèµ å“ï¼Œéå¸¸æ„‰å¿«çš„è´­ç‰©ï¼\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å‘³é“å¾ˆå¹³æ·¡ï¼Œå¾ˆå¥½ç”¨ï¼Œä¸‹æ¬¡è¿˜ä¼šä¹°çš„ã€‚å¥½è¯„ï¼Œå¥½è¯„ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: å›è´­çš„ï¼Œä¹‹å‰æ˜¯åœ¨å…ç¨åº—ä¹°çš„\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¤ªç¾äº†æŠŠ...\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: å¾ˆå¥½çœ‹è–„è·ç»¿çš„å£³å¾ˆä»™å¾ˆå–œæ¬¢ä»£è¨€äººåƒçºå®å®çš„æ¨èğŸ’—\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è¿˜å¯ä»¥ï¼Œç­‰äº†å¥½ä¹…ï¼ŒåŒåä¸€ä¸‹æ‰‹çš„ï¼Œè›®å¥½ç”¨\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: åŒåä¸€ä¹°çš„ï¼Œä¹Ÿå¾ˆåˆ’ç®—ï¼Œèµ å“å¾ˆå–œæ¬¢ï¼Œé€çš„åŒ–å¦†åŒ…å¥½çœ‹\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ä¹°ç»™é‚£ä¸ªå¥¹çš„ï¼Œå¾ˆå¥½ï¼å¥¹éå¸¸å–œæ¬¢â•®(ï¿£â–½ï¿£\")â•­\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è™½ç„¶æ˜¯ä¸ªå£çº¢ä½†æ˜¯åŒ…è£…å¾ˆèµ°å¿ƒäº†è®²çœŸçš„\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¿ƒåŠ¨\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è¿™é¢œè‰²ï¼Œæ£’çš„å¾ˆå˜›ã€‚å¾ˆæ˜¾ç™½å‘¢ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¥½çœ‹çš„\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: å¤§ç‰Œå°±æ˜¯å¤§ç‰Œï¼ŒçœŸçš„å¾ˆå¥½ï¼Œå¾ˆæœå¸–\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æ„Ÿè§‰è¿˜å¥½å§ã€‚åæ­£æ˜¯é€äººçš„ã€‚å¥³å­©å­æ”¶åˆ°çš„æ—¶å€™å¤§å«äº†ä¸€å£°å¦ˆè€¶ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¥³å‹æ‹¿åˆ°äº†å¾ˆå¼€å¿ƒï¼Œé¢œè‰²ä¹Ÿè¶…å–œæ¬¢ï¼Œä¸Šè‰²å¾ˆç¾\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è¶…çº§å¥½çœ‹è¶…çº§å¥½çœ‹æ¶‚åœ¨å˜´å·´ä¸Šé‚£å¥½çœ‹çš„é¢œè‰²çœŸçš„æ‹ä¸å‡ºæ¥å¥½çœ‹åˆ°çˆ†ç‚¸\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: åˆšå¼€å§‹ä¸Šè„¸æœ‰äº›å‡ç™½ï¼Œä¸€æ®µæ—¶é—´ä¹‹åä¼šè¶Šæ¥è¶Šè‡ªç„¶ï¼Œå¦†é¢å¾ˆæœå¸–ï¼Œä¹Ÿä¸ä¼šå¹²ï¼Œå¾ˆå¥½\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®º!\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è¿˜æ²¡ç”¨ã€‚ä¹‹å‰çš„è¢«å„¿å­æ‘”ç¢äº†ã€‚ã€‚åœ¨æ——èˆ°åº—ä¹°äº†å¾ˆå¤šä¸œè¥¿ï¼Œå¸Œæœ›æ­£å“ã€‚å¸Œæœ›ä¸€å¦‚æ—¢å¾€å¥½ç”¨ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ä¸œè¥¿å¾ˆæ£’ï¼Œæ— è®ºæ˜¯å¤–è§‚è¿˜æ˜¯å“è´¨ï¼Œéƒ½æ˜¯éå¸¸ä¸é”™çš„ã€‚ç•™é¦™å¯ä»¥ç•™å¾ˆä¹…ï¼Œå–·ä¸€æ¬¡å¯ä»¥ç”¨ä¸€æ•´å¤©ï¼Œä½†ä¸å®œå–·å¤šäº†ï¼Œä¸€ç‚¹ç‚¹å°±è¶³å¤Ÿäº†ã€‚æ”¾åœ¨å¯å®¤çš„æ¡Œå­ä¸Šï¼Œæ„Ÿè§‰é€¼æ ¼éƒ½é«˜äº†ç‚¹ï¼Œç»™å®¤å‹æ¨èäº†ï¼Œå¥¹ä»¬ä¹Ÿè¯´ä¸é”™çš„ï¼Œå¾ˆå®æƒ å¾ˆå®ç”¨\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é¢œå€¼è¶…é«˜ï¼ï¼ï¼é¢œè‰²éå¸¸æ­£ï¼ï¼å‰é¢ä¹°äº†ä¸€æ”¯è¢«åŒäº‹æŠ¢èµ°æ‹¿å»å½“ä¸ƒå¤•èŠ‚ç¤¼ç‰©äº†ğŸ˜‚ğŸ˜‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é¢œå€¼é«˜\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ä¸­é—´å‡ºäº†ä¸€ç‚¹å°æ„å¤–ï¼Œç»“æœè¿˜æ˜¯å¾ˆå®Œç¾çš„ğŸ™‚ğŸ™‚å¾ˆå¼€å¿ƒï¼Œå®¢æœä¹Ÿå¾ˆå¥½ğŸ‘\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¥½ç”¨çš„å›è´­äº§å“ï¼é€çš„èµ å“ä¹Ÿå¾ˆå–œæ¬¢\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¸®æœ‹å‹ä¹°çš„ï¼Œä¸é”™ã€‚ã€‚ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "æ‚¨è¾“å…¥çš„å¥å­æœ‰æ±‰å­—ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè¯·é‡æ–°è¾“å…¥ï¼\n",
            "ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯ä¸ºï¼š'é½'.\n",
            "è¾“å…¥è¯­å¥: éå¸¸éå¸¸æ»¡æ„ï¼Œæ˜¯å› ä¸ºå†œå†œæ‰ä¹°çš„ï¼Œä¸è¿‡çœŸçš„å¾ˆå–œæ¬¢ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¥³æœ‹å‹å¾ˆå–œæ¬¢\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é¢œè‰²b20çš„ï¼Œä¸é€‚åˆæˆ‘ï¼Œ400è½¬äº†ï¼Œç”¨è¿‡ä¸‰æ¬¡\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: ç‰¹åˆ«æ¼‚äº®ï¼Œé€ç»™å¦ˆå¦ˆçš„ï¼Œé¢œè‰²å¾ˆæ­£ï¼Œç•™è‰²åº¦å¾ˆå¥½\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®º!\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æƒ³ç»™ä¸ªå¥½è¯„çœ‹å¥³æœ‹å‹å–œä¸å–œæ¬¢\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®º!\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¾ˆæ£’\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: è§è¿‡æœ€ç¨€çš„ç²‰åº•æ¶²äº†ä¸è¿‡å¾ˆæ¶¦çš„å®Œå…¨æ²¡æœ‰å¡ç²‰\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ä¹°æ¥é€äººçš„ã€‚æœç„¶æ¼‚äº®ï¼å–œæ¬¢ï¼\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é€Ÿåº¦è¶…çº§å¿«ï¼ŒåŒåä¸€è´­ç‰©ç‹‚æ¬¢èŠ‚æ´»åŠ¨ä¹°çš„,ä¸‹åˆå°±æ”¶åˆ°è´§äº†ï¼Œè¿˜é€äº†ä¸€æ”¯å£çº¢å°æ ·ã€‚ç¬¬ä¸€æ¬¡ä½¿ç”¨ï¼Œé™å¾…ä½¿ç”¨æ•ˆæœã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æ”¯æŒä»£è¨€äººåˆèƒ½æé«˜ç”Ÿæ´»è´¨é‡\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: ä¸€ç›´åœ¨æ‰¾è¿™æ¬¾å°é“ƒé“›çš„æ•£ç²‰ï¼Œæ–¹ä¾¿æºå¸¦ï¼Œå¤å¤©èƒŒå°åŒ…ï¼Œè¿™ä¸ªå°ºå¯¸åˆšå¥½å¡è¿›åŒ…é‡Œã€‚å”‡è†è´¨åœ°ç»†è…»ï¼Œå®¹æ˜“æ¨å¼€ï¼Œä¸å¡å”‡çº¹ï¼Œè–„æ¶‚åšæ¶‚éƒ½å¥½ç”¨ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®º!\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é¢œè‰²niceï¼Œæ»‹æ¶¦åº¦ä¸€èˆ¬ï¼Œè¦æ¶‚æŠ¹å”‡è†ã€‚å¯èƒ½æ˜¯æˆ‘çš„å”‡æ¯”è¾ƒå¹²\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è¿™æ¬¾å¹²ç²‰ç”¨ç€ä¸é”™ä¼šå›è´­é€‚åˆæˆ‘è¿™ç§ç™½çš™è‚¤è‰²çš„ä¸è„±å¦†æˆ‘æœ¬äººä¹Ÿå¾ˆå–œæ¬¢è¿™ä¸ªæ–¹ç›’å­å¦‚æœèƒ½æœ‰ä¸€ä¸ªç²‰å¥—å°±æ›´å¥½äº†ç‰©æµä¹Ÿå¾ˆå¿«\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "æ‚¨è¾“å…¥çš„å¥å­æœ‰æ±‰å­—ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè¯·é‡æ–°è¾“å…¥ï¼\n",
            "ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯ä¸ºï¼š'à©­'.\n",
            "è¾“å…¥è¯­å¥: ä¸å‡ç™½ï¼Œç²‰æ‰‘å½¢çŠ¶ç”¨ç€ç¡®å®å¾ˆæ–¹ä¾¿ï¼Œè€Œä¸”å¦†å®¹å¯ä»¥ä¿æŒå¾ˆä¹…ï¼Œé®ç‘•åŠ›åº¦ç›¸æ¯”è¾ƒå…¶ä»–æ°”å«æ¥è¯´å¾ˆå¥½äº†ï¼Œå…¶å®æ˜¯æ›´å–œæ¬¢ç²‰è‰²çš„ç›’å­ï¼Œä½†æ˜¯æˆ‘è„¸ä¸Šç‘•ç–µæ¯”è¾ƒå¤šï¼Œåªèƒ½ä¹°çº¢è‰²ï¼Œè¿˜é€äº†ä¸€äº†é¦™æ°´å°æ ·ï½å“ˆå“ˆå“ˆï½å…¶å®æƒ³è¦çº¢ç®¡400å°æ ·ï¼Œä¸è¿‡åº”è¯¥æ˜¯ä¹‹å‰é¢„å®šçš„äººé€çš„å§å“ˆå“ˆå“ˆå“ˆï½\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æˆ‘å®¶å¶åƒä»£è¨€çš„æˆ‘å°±ä¹°äº†è¿˜æ²¡ç”¨æ‘†ç€ä¹Ÿå¥½çœ‹\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: çœ‹æ ·å­æŒºå¥½çš„ï¼Œç»™åª³å¦‡ä¹°çš„-è¿˜æ²¡ç”¨å‘¢\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: ç²‰æ‰‘åšçš„å¤ªç²—ç³™äº†æ ¹æœ¬æ²¡æœ‰é¦™å¥ˆå„¿é‚£ä¹ˆç²¾è‡´ã€‚ç™½çš„å¾ˆè‡ªç„¶ï¼Œä¹Ÿå¾ˆæ°´æ¶¦.ä½†æ˜¯ä¸çŸ¥é“ä¸ºä»€ä¹ˆæ€»æ„Ÿè§‰æœ‰ç‚¹å‡æ»‘çš„æ„Ÿè§‰ã€‚æ¶‚èµ·æ¥æ•´ä¸ªè„¸æ²¹å…‰å‘äº®.é€‚åˆå¹²çš®.ä¸è¿‡æˆ‘éƒ½ç”¨æ¥è¡¥å¦†çš„ï¼Œè¿˜ä¸é”™.\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®º!\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®º!\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å®è´ä¸é”™å“¦\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å®è´ï¼Œå¾ˆå¥½\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è¶…çº§ç¾ä¸½äº†\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: é¢œè‰²å¾ˆæ­£ç•¥å¾®æœ‰ç‚¹ç²˜æ¯\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é€ç»™å¥³æœ‹å‹çš„ï¼Œå¸Œæœ›å¥¹ä¼šå–œæ¬¢\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: åŒ…è£…å¾ˆå¥½æ˜¾è‰²åº¦å¾ˆé«˜å¾ˆé¥±å’Œé¢œè‰²ç‰¹åˆ«å¥½çœ‹\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å–œæ¬¢å–œæ¬¢è¶…å–œæ¬¢ï¼Œå‘³é“è¯´ä¸å‡ºçš„å¥½é—»ï¼Œè‡ªå·±éƒ½è¦è¢«è¿·é†‰äº†ï¼ä¹°çš„å€¼äº†\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æˆ‘å®¶å°ç‹—å­è§‰å¾—è¿˜å¯ä»¥\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: å¿«é€’å¾ˆå¿«ï¼Œè¿™ä¸ªé¢œè‰²ä¸æ˜¯ç‰¹åˆ«çš„è‰³ä¸½ï¼Œæˆ‘è¿˜æ˜¯å–œæ¬¢å¾ˆçº¢å¾ˆçº¢çš„ï¼Œå“ˆå“ˆ\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®º!\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ç¬¬ä¸€æ¬¡ä¹°äº†è¿™ä¹ˆè´µçš„æ°”å«ï¼Œä½†æ˜¯ä¸ªäººè§‰å¾—ç‰©æœ‰æ‰€å€¼ï¼Œæˆ‘çš®è‚¤å±äºæ—©ä¸Šç™½ï¼Œåˆ°ä¸‹åˆå°±æš—æ²‰çš„é‚£ç§ï¼Œæ˜¨å¤©å››ç‚¹å¤šä¸Šå¦†ï¼Œåˆ°æ™šä¸Šåç‚¹åŠå·¦å³ï¼Œåªæœ‰é¼»å­é‚£é‡Œæœ‰ç‚¹æš—æ²‰ï¼Œå…¶ä»–éƒ½åœ¨çº¿ï¼Œä¸å¾—ä¸è¯´ä¸€ä¸‹èµ é€çš„æ˜å½©ç²‰åº•æ¶²ï¼Œè¶…çº§å¥½ç”¨å•Šï¼Œä¸€ç‚¹ç‚¹å°±å¯ä»¥æ¨å…¨è„¸ï¼Œè€Œä¸”ä¸å‡ç™½ï¼Œå¾ˆå¸–åˆæˆ‘çš„çš®è‚¤ï¼Œæ„Ÿè§‰çš®è‚¤è¶…å¥½ï¼Œå¾ˆç»†è…»ã€‚å¯¹äº†ï¼Œè¿˜æœ‰é¦™æ°´ï¼Œå¾ˆå¥½é—»ï¼Œè™½ç„¶ä¸æ˜¯æˆ‘æƒ³è¦çš„é»‘é¸¦ç‰‡ï¼Œä½†æ˜¯åè½¬å·´é»ä¹Ÿå¾ˆä¸é”™å“¦ï¼ä¸‹ä¸€æ­¥æœ‰ç‚¹æ‰“ç®—å…¥æ‰‹æ˜å½©ç²‰åº•æ¶²å•¦ï¼å“ˆå“ˆï¼Œè¢«æ¨æ ‘æ—åœˆç²‰å•¦ï¼\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¥½çœ‹ï¼ŒçœŸä¸é”™ï¼Œé€‚åˆå†¬å¤©\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ä¸é”™å¥³æœ‹å‹å¾ˆå–œæ¬¢é‚£æˆ‘å°±å¾ˆå–œæ¬¢\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: åˆ«äººå–œæ¬¢å°±å¥½æ”¶ä¸‹æˆ‘å°±å¼€å¿ƒï¼ åšä¸ªå¥½äººæŒºå¥½\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: éå¸¸å¥½ç”¨å¾ˆå–œæ¬¢ğŸ˜˜\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: åŒ…è£…é«˜ç«¯å¤§æ°”ä¸Šæ¡£æ¬¡ï¼Œç”¨èµ·æ¥ä¹Ÿéå¸¸éå¸¸å¥½ï¼Œ\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: è¿˜æ˜¯è§‰å¾—æ¨æ ‘æ—å¥½ç”¨ï¼Œä»ç¬¬ä¸€åªå¼€å§‹å°±ä¸€å‘ä¸å¯æ”¶æ‹¾ï¼Œå†å¤šéƒ½è§‰å¾—ä¸å¤ŸğŸ˜„\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: åŒ…è£…æ²¡è¯è¯´ï¼Œä½†æ˜¯å£çº¢â€¦â€¦ä¸€èˆ¬èˆ¬æœ‰ç‚¹å°å¤±æœ›\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: éå¸¸å¥½ï¼Œç—˜å°å‘æ´¼åŸºæœ¬éƒ½èƒ½é®æ‰ä¸€å¤©ä¸Šç­8å°æ—¶ï¼Œæ²¹æ€§çš®è‚¤è½»å¾®å‡ºæ²¹ ysl ä»æœªè®©æˆ‘å¤±æœ›è¿‡\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å‰å¥³å‹è¯´å¾ˆå¥½çœ‹\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: æŒºå¥½çš„ï¼Œè¿˜æ²¡ç”¨ï¼Œæ„Ÿè§‰å¾ˆä¸é”™ï¼Œéå¸¸å¥½ï¼ŒåŒ…è£…å¾ˆå¥½ï¼Œå¿«é€’ç»™åŠ›ï¼ŒæœåŠ¡ä¹Ÿå¾ˆèµï¼Œæ¨èè´­ä¹°ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: é›¾é¢é«˜å…‰æ•ˆæœä¸é”™\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: å¥½çœ‹å¥½çœ‹ï¼ŒåŒ…è£…ä¹Ÿå¥½çœ‹ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æˆ‘ä¹°è¿™æ¬¾ç²‰åº•æ¶²è¿˜æ˜¯ä¸é”™çš„ï¼Œç”¨èµ·æ¥å¾ˆè½»è–„ï¼Œè‡ªç„¶ï¼Œæˆ‘æ˜¯å¹³æ—¶ä¸Šç­å°±æ¶‚ä¸Šæ·¡æ·¡çš„ä¸€å±‚ï¼Œæˆ‘å–œæ¬¢è‡ªç„¶ï¼Œç‰©æµå€’æ˜¯å¾ˆå¿«ï¼Œæˆ‘åœ¨æƒ³ï¼Œåº”è¯¥æ˜¯æ­£å“ï¼Œæ¯•ç«Ÿæ˜¯å®˜ç½‘å˜›ğŸ˜Šï¼Œæˆ‘é€‰æ‹©è¿™æ¬¾æ˜¯å®¢æœä¸ºæˆ‘æ¨èçš„ï¼Œè¿˜æŒºåˆé€‚æˆ‘ã€‚ä¸ç®¡å¥½çš„ä¸å¥½çš„ï¼Œç”¨äº†å†è¯´å§ï¼Œæ—¶é—´èƒ½è¯æ˜ä¸€åˆ‡ï¼Œæˆ‘åˆ°æ—¶å€™å»ä¸“æŸœå¯¹æ¯”ä¸€ä¸‹å¦‚æœæ˜¯æ­£å“æˆ‘å†è¦è¿½è¯„ã€‚å¸Œæœ›æ²¡æœ‰è®©æˆ‘å¤±æœ›å§\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ç”·ç¥¨å–œæ¬¢ï¼Œå•¥éƒ½å¥½\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: éå¸¸æ£’ã€‚çœ‹ç€å°±å¾ˆé«˜å¤§ä¸Šã€‚å–œæ¬¢ã€‚\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æš‚æ—¶ä¸çŸ¥é“\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: ä»€ä¹ˆä¸è„±è‰²ï¼Œéª—äººçš„ï¼Œè¿˜è¿™ä¹ˆè´µ\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: æ­£é¢\n",
            "è¾“å…¥è¯­å¥: è´§ç‰©æ”¶åˆ°äº†ï¼Œå¾ˆä¸é”™ï¼Œä¸ä»‹ç»ç›¸ç¬¦åˆï¼Œå¿«é€’ä¹Ÿå¾ˆå¿«ï¼Œå¾ˆæ»¡æ„\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: æŒºå¥½çš„ï¼Œæ»‹æ¶¦ï¼\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n",
            "è¾“å…¥è¯­å¥: ä¸œè¥¿å¾ˆå¥½ï¼Œå¥³æœ‹å‹å¾ˆå–œæ¬¢ï¼Œæ»¡åˆ†\n",
            "æƒ…æ„Ÿé¢„æµ‹ç»“æœ: è´Ÿé¢\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB5d9NEzEf8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}