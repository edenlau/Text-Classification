{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beauty LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edenlau/Text-Classification/blob/master/Beauty_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3u5gbePAKPs",
        "colab_type": "code",
        "outputId": "b31cc352-a7e9-4b03-c8e9-71200729cae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyU2QX7ApUn",
        "colab_type": "code",
        "outputId": "87aa5861-9583-410e-b5ca-2285d43d6a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /gdrive/My Drive/LSTM\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import font_manager\n",
        "from itertools import accumulate\n",
        "\n",
        "# 设置matplotlib绘图时的字体\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "my_font = font_manager.FontProperties(fname=\"./NotoSansSC-Regular.otf\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH03vtvo_zil",
        "colab_type": "code",
        "outputId": "aa1130bd-62c5-4d1c-ae4b-739851d32bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# 统计句子长度及长度出现的频数\n",
        "filepath = \"./beauty_corpus.csv\"\n",
        "df = pd.read_csv(filepath)\n",
        "print(df.groupby('label')['label'].count())\n",
        "\n",
        "df['length'] = df['evaluation'].apply(lambda x: len(x))\n",
        "len_df = df.groupby('length').count()\n",
        "sent_length = len_df.index.tolist()\n",
        "sent_freq = len_df['evaluation'].tolist()\n",
        "\n",
        "# 绘制句子长度及出现频数统计图\n",
        "plt.bar(sent_length, sent_freq)\n",
        "plt.title(\"句子长度及出现频数统计图\", fontproperties=my_font)\n",
        "plt.xlabel(\"句子长度\", fontproperties=my_font)\n",
        "plt.ylabel(\"句子长度出现的频数\", fontproperties=my_font)\n",
        "plt.savefig(\"句子长度及出现频数统计图.png\")\n",
        "#files.download(\"句子长度及出现频数统计图.png\")\n",
        "plt.close()\n",
        "\n",
        "# 绘制句子长度累积分布函数(CDF)\n",
        "sent_pentage_list = [(count/sum(sent_freq)) for count in accumulate(sent_freq)]\n",
        "\n",
        "# 绘制CDF\n",
        "plt.plot(sent_length, sent_pentage_list)\n",
        "\n",
        "# 寻找分位点为quantile的句子长度\n",
        "quantile = 0.91\n",
        "#print(list(sent_pentage_list))\n",
        "for length, per in zip(sent_length, sent_pentage_list):\n",
        "    if round(per, 2) == quantile:\n",
        "        index = length\n",
        "        break\n",
        "print(\"\\n分位点为%s的句子长度:%d.\" % (quantile, index))\n",
        "\n",
        "# 绘制句子长度累积分布函数图\n",
        "plt.plot(sent_length, sent_pentage_list)\n",
        "plt.hlines(quantile, 0, index, colors=\"c\", linestyles=\"dashed\")\n",
        "plt.vlines(index, 0, quantile, colors=\"c\", linestyles=\"dashed\")\n",
        "plt.text(0, quantile, str(quantile))\n",
        "plt.text(index, 0, str(index))\n",
        "plt.title(\"句子长度累积分布函数图\", fontproperties=my_font)\n",
        "plt.xlabel(\"句子长度\", fontproperties=my_font)\n",
        "plt.ylabel(\"句子长度累积频率\", fontproperties=my_font)\n",
        "plt.savefig(\"句子长度累积分布函数图.png\")\n",
        "#files.download(\"句子长度累积分布函数图.png\") \n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label\n",
            "中性     1105\n",
            "正面    15906\n",
            "负面      823\n",
            "Name: label, dtype: int64\n",
            "\n",
            "分位点为0.91的句子长度:80.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYd3Bv2O1mIz",
        "colab_type": "code",
        "outputId": "bb941d70-71c9-4e0e-85f6-fb994f65a301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1058
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import np_utils, plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 导入数据\n",
        "# 文件的数据中，特征为evaluation, 类别为label.\n",
        "def load_data(filepath, input_shape=20):\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # 标签及词汇表\n",
        "    labels, vocabulary = list(df['label'].unique()), list(df['evaluation'].unique())\n",
        "\n",
        "    # 构造字符级别的特征\n",
        "    string = ''\n",
        "    for word in vocabulary:\n",
        "        string += word\n",
        "\n",
        "    vocabulary = set(string)\n",
        "\n",
        "    # 字典列表\n",
        "    word_dictionary = {word: i+1 for i, word in enumerate(vocabulary)}\n",
        "    with open('word_dict.pk', 'wb') as f:\n",
        "        pickle.dump(word_dictionary, f)\n",
        "    inverse_word_dictionary = {i+1: word for i, word in enumerate(vocabulary)}\n",
        "    label_dictionary = {label: i for i, label in enumerate(labels)}\n",
        "    with open('label_dict.pk', 'wb') as f:\n",
        "        pickle.dump(label_dictionary, f)\n",
        "    output_dictionary = {i: labels for i, labels in enumerate(labels)}\n",
        "\n",
        "    vocab_size = len(word_dictionary.keys()) # 词汇表大小\n",
        "    label_size = len(label_dictionary.keys()) # 标签类别数量\n",
        "\n",
        "    # 序列填充，按input_shape填充，长度不足的按0补充\n",
        "    x = [[word_dictionary[word] for word in sent] for sent in df['evaluation']]\n",
        "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
        "    y = [[label_dictionary[sent]] for sent in df['label']]\n",
        "    y = [np_utils.to_categorical(label, num_classes=label_size) for label in y]\n",
        "    y = np.array([list(_[0]) for _ in y])\n",
        "\n",
        "    return x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary\n",
        "\n",
        "# 创建深度学习模型， Embedding + LSTM + Softmax.\n",
        "def create_LSTM(n_units, input_shape, output_dim, filepath):\n",
        "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size + 1, output_dim=output_dim,\n",
        "                        input_length=input_shape, mask_zero=True))\n",
        "    model.add(LSTM(n_units, input_shape=(x.shape[0], x.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(label_size, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    plot_model(model, to_file='./model_lstm.png', show_shapes=True)\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# 模型训练\n",
        "def model_train(input_shape, filepath, model_save_path):\n",
        "\n",
        "    # 将数据集分为训练集和测试集，占比为9:1\n",
        "    # input_shape = 100\n",
        "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)\n",
        "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
        "\n",
        "    # 模型输入参数，需要自己根据需要调整\n",
        "    n_units = 100\n",
        "    batch_size = 32\n",
        "    epochs = 5\n",
        "    output_dim = 20\n",
        "\n",
        "    # 模型训练\n",
        "    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)\n",
        "    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    # 模型保存\n",
        "    lstm_model.save(model_save_path)\n",
        "\n",
        "    N = test_x.shape[0]  # 测试的条数\n",
        "    predict = []\n",
        "    label = []\n",
        "    for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
        "        sentence = [inverse_word_dictionary[i] for i in test_x[start] if i != 0]\n",
        "        y_predict = lstm_model.predict(test_x[start:end])\n",
        "        label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
        "        label_true = output_dictionary[np.argmax(test_y[start:end])]\n",
        "        print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
        "        predict.append(label_predict)\n",
        "        label.append(label_true)\n",
        "\n",
        "    acc = accuracy_score(predict, label) # 预测准确率\n",
        "    print('模型在测试集上的准确率为: %s.' % acc)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    filepath = './beauty_corpus.csv'\n",
        "    input_shape = 180\n",
        "    model_save_path = './beauty_corpus_model.h5'\n",
        "    model_train(input_shape, filepath, model_save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 03:36:52.111222 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0618 03:36:52.142178 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0618 03:36:52.151009 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0618 03:36:52.393677 140485073356672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0618 03:36:52.428653 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0618 03:36:52.437150 140485073356672 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0618 03:36:52.464080 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0618 03:36:52.486501 140485073356672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 180, 20)           67060     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               48400     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 115,864\n",
            "Trainable params: 115,864\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "16051/16051 [==============================] - 188s 12ms/step - loss: 0.3340 - acc: 0.9137\n",
            "Epoch 2/5\n",
            "16051/16051 [==============================] - 179s 11ms/step - loss: 0.2078 - acc: 0.9278\n",
            "Epoch 3/5\n",
            " 2816/16051 [====>.........................] - ETA: 2:23 - loss: 0.1943 - acc: 0.9315"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c8a47d41ed40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./beauty_corpus_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-c8a47d41ed40>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(input_shape, filepath, model_save_path)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# 模型训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# 模型保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl927EBSkHSV",
        "colab_type": "code",
        "outputId": "df76aa74-5ecb-4403-b137-52c299f06e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Import the necessary modules\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# 导入字典\n",
        "with open('word_dict.pk', 'rb') as f:\n",
        "    word_dictionary = pickle.load(f)\n",
        "with open('label_dict.pk', 'rb') as f:\n",
        "    output_dictionary = pickle.load(f)\n",
        "\n",
        "try:\n",
        "    # 数据预处理\n",
        "    input_shape = 180\n",
        "    sent = \"作为一个男的，我是不懂口红怎么样的，买来送人的，但作为一个牌子的旗舰店包装都是这么简陋的吗\"\n",
        "    x = [[word_dictionary[word] for word in sent]]\n",
        "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
        "\n",
        "    # 载入模型\n",
        "    model_save_path = './corpus_model.h5'\n",
        "    lstm_model = load_model(model_save_path)\n",
        "\n",
        "    # 模型预测\n",
        "    y_predict = lstm_model.predict(x)\n",
        "    label_dict = {v:k for k,v in output_dictionary.items()}\n",
        "    print('输入语句: %s' % sent)\n",
        "    print('情感预测结果: %s' % label_dict[np.argmax(y_predict)])\n",
        "\n",
        "except KeyError as err:\n",
        "    print(\"您输入的句子有汉字不在词汇表中，请重新输入！\")\n",
        "    print(\"不在词汇表中的单词为：%s.\" % err)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "输入语句: 作为一个男的，我是不懂口红怎么样的，买来送人的，但作为一个牌子的旗舰店包装都是这么简陋的吗\n",
            "情感预测结果: 正面\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j3JrrTg1o0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bdf = pd.read_csv(\"./ECData.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzr9lVEB1-ME",
        "colab_type": "code",
        "outputId": "a503f00b-35d4-4776-e948-085d35ed849a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2927
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Import the necessary modules\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# 导入字典\n",
        "with open('word_dict.pk', 'rb') as f:\n",
        "    word_dictionary = pickle.load(f)\n",
        "with open('label_dict.pk', 'rb') as f:\n",
        "    output_dictionary = pickle.load(f)\n",
        "\n",
        "sbdf = bdf.sample(frac=0.01) # set sampling ratio for evaluation\n",
        "    \n",
        "for i, j in sbdf.iterrows(): \n",
        "    sent = sbdf.loc[i, \"Comment\"]\n",
        "    try:\n",
        "        # 数据预处理\n",
        "        input_shape = 180\n",
        "        #sent = \"作为一个男的，我是不懂口红怎么样的，买来送人的，但作为一个牌子的旗舰店包装都是这么简陋的吗\"\n",
        "        x = [[word_dictionary[word] for word in sent]]\n",
        "        x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
        "\n",
        "        # 载入模型\n",
        "        model_save_path = './corpus_model.h5'\n",
        "        lstm_model = load_model(model_save_path)\n",
        "\n",
        "        # 模型预测\n",
        "        y_predict = lstm_model.predict(x)\n",
        "        label_dict = {v:k for k,v in output_dictionary.items()}\n",
        "        print('输入语句: %s' % sent)\n",
        "        print('情感预测结果: %s' % label_dict[np.argmax(y_predict)])\n",
        "\n",
        "    except KeyError as err:\n",
        "        print(\"您输入的句子有汉字不在词汇表中，请重新输入！\")\n",
        "        print(\"不在词汇表中的单词为：%s.\" % err)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "输入语句: 很快就收到，很喜欢味道香香的。颜色挺像mac的diva，不过ysl的唇釉我真的很喜欢不干而且不沾杯。上嘴我是无论什么色都会偏紫，一定涂手上就是中间那个颜色。\n",
            "情感预测结果: 负面\n",
            "输入语句: 相当满意的一次购物\n",
            "情感预测结果: 正面\n",
            "输入语句: 好看。喜欢。\n",
            "情感预测结果: 负面\n",
            "输入语句: 颜值真的超高，包装也很高大上，很喜欢，控油效果真的不错，但隐形毛孔感觉作用不大，也有可能个人不怎么会用，然后还有就可以定妆也不知道怎么用，总的还是比较满意\n",
            "情感预测结果: 负面\n",
            "输入语句: 好看，还有股悠悠的香味～\n",
            "情感预测结果: 负面\n",
            "输入语句: 双11购的这么快就收到了太美了也谢谢快递员，包装完美，颜色完美，还收到了好多赠品，非常愉快的购物！双11购的这么快就收到了太美了也谢谢快递员，包装完美，颜色完美，还收到了好多赠品，非常愉快的购物！\n",
            "情感预测结果: 负面\n",
            "输入语句: 味道很平淡，很好用，下次还会买的。好评，好评。\n",
            "情感预测结果: 正面\n",
            "输入语句: 回购的，之前是在免税店买的\n",
            "情感预测结果: 负面\n",
            "输入语句: 太美了把...\n",
            "情感预测结果: 正面\n",
            "输入语句: 很好看薄荷绿的壳很仙很喜欢代言人千玺宝宝的推荐💗\n",
            "情感预测结果: 负面\n",
            "输入语句: 还可以，等了好久，双十一下手的，蛮好用\n",
            "情感预测结果: 负面\n",
            "输入语句: 双十一买的，也很划算，赠品很喜欢，送的化妆包好看\n",
            "情感预测结果: 负面\n",
            "输入语句: 买给那个她的，很好！她非常喜欢╮(￣▽￣\")╭\n",
            "情感预测结果: 负面\n",
            "输入语句: 虽然是个口红但是包装很走心了讲真的\n",
            "情感预测结果: 负面\n",
            "输入语句: 心动\n",
            "情感预测结果: 负面\n",
            "输入语句: 这颜色，棒的很嘛。很显白呢。\n",
            "情感预测结果: 负面\n",
            "输入语句: 好看的\n",
            "情感预测结果: 正面\n",
            "输入语句: 大牌就是大牌，真的很好，很服帖\n",
            "情感预测结果: 负面\n",
            "输入语句: 感觉还好吧。反正是送人的。女孩子收到的时候大叫了一声妈耶。\n",
            "情感预测结果: 负面\n",
            "输入语句: 女友拿到了很开心，颜色也超喜欢，上色很美\n",
            "情感预测结果: 负面\n",
            "输入语句: 超级好看超级好看涂在嘴巴上那好看的颜色真的拍不出来好看到爆炸\n",
            "情感预测结果: 正面\n",
            "输入语句: 刚开始上脸有些假白，一段时间之后会越来越自然，妆面很服帖，也不会干，很好\n",
            "情感预测结果: 负面\n",
            "输入语句: 此用户没有填写评论!\n",
            "情感预测结果: 负面\n",
            "输入语句: 还没用。之前的被儿子摔碎了。。在旗舰店买了很多东西，希望正品。希望一如既往好用。\n",
            "情感预测结果: 负面\n",
            "输入语句: 东西很棒，无论是外观还是品质，都是非常不错的。留香可以留很久，喷一次可以用一整天，但不宜喷多了，一点点就足够了。放在寝室的桌子上，感觉逼格都高了点，给室友推荐了，她们也说不错的，很实惠很实用\n",
            "情感预测结果: 负面\n",
            "输入语句: 颜值超高！！！颜色非常正！！前面买了一支被同事抢走拿去当七夕节礼物了😂😂\n",
            "情感预测结果: 负面\n",
            "输入语句: 颜值高\n",
            "情感预测结果: 负面\n",
            "输入语句: 中间出了一点小意外，结果还是很完美的🙂🙂很开心，客服也很好👍\n",
            "情感预测结果: 负面\n",
            "输入语句: 好用的回购产品！送的赠品也很喜欢\n",
            "情感预测结果: 负面\n",
            "输入语句: 帮朋友买的，不错。。。\n",
            "情感预测结果: 负面\n",
            "您输入的句子有汉字不在词汇表中，请重新输入！\n",
            "不在词汇表中的单词为：'齁'.\n",
            "输入语句: 非常非常满意，是因为农农才买的，不过真的很喜欢。\n",
            "情感预测结果: 负面\n",
            "输入语句: 女朋友很喜欢\n",
            "情感预测结果: 负面\n",
            "输入语句: 颜色b20的，不适合我，400转了，用过三次\n",
            "情感预测结果: 正面\n",
            "输入语句: 特别漂亮，送给妈妈的，颜色很正，留色度很好\n",
            "情感预测结果: 负面\n",
            "输入语句: 此用户没有填写评论!\n",
            "情感预测结果: 负面\n",
            "输入语句: 想给个好评看女朋友喜不喜欢\n",
            "情感预测结果: 正面\n",
            "输入语句: 此用户没有填写评论!\n",
            "情感预测结果: 负面\n",
            "输入语句: 很棒\n",
            "情感预测结果: 正面\n",
            "输入语句: 见过最稀的粉底液了不过很润的完全没有卡粉\n",
            "情感预测结果: 负面\n",
            "输入语句: 买来送人的。果然漂亮！喜欢！\n",
            "情感预测结果: 负面\n",
            "输入语句: 速度超级快，双十一购物狂欢节活动买的,下午就收到货了，还送了一支口红小样。第一次使用，静待使用效果。\n",
            "情感预测结果: 负面\n",
            "输入语句: 支持代言人又能提高生活质量\n",
            "情感预测结果: 正面\n",
            "输入语句: 一直在找这款小铃铛的散粉，方便携带，夏天背小包，这个尺寸刚好塞进包里。唇膏质地细腻，容易推开，不卡唇纹，薄涂厚涂都好用。\n",
            "情感预测结果: 负面\n",
            "输入语句: 此用户没有填写评论!\n",
            "情感预测结果: 负面\n",
            "输入语句: 颜色nice，滋润度一般，要涂抹唇膏。可能是我的唇比较干\n",
            "情感预测结果: 负面\n",
            "输入语句: 这款干粉用着不错会回购适合我这种白皙肤色的不脱妆我本人也很喜欢这个方盒子如果能有一个粉套就更好了物流也很快\n",
            "情感预测结果: 负面\n",
            "您输入的句子有汉字不在词汇表中，请重新输入！\n",
            "不在词汇表中的单词为：'੭'.\n",
            "输入语句: 不假白，粉扑形状用着确实很方便，而且妆容可以保持很久，遮瑕力度相比较其他气垫来说很好了，其实是更喜欢粉色的盒子，但是我脸上瑕疵比较多，只能买红色，还送了一了香水小样～哈哈哈～其实想要红管400小样，不过应该是之前预定的人送的吧哈哈哈哈～\n",
            "情感预测结果: 负面\n",
            "输入语句: 我家偶像代言的我就买了还没用摆着也好看\n",
            "情感预测结果: 负面\n",
            "输入语句: 看样子挺好的，给媳妇买的-还没用呢\n",
            "情感预测结果: 正面\n",
            "输入语句: 粉扑做的太粗糙了根本没有香奈儿那么精致。白的很自然，也很水润.但是不知道为什么总感觉有点假滑的感觉。涂起来整个脸油光发亮.适合干皮.不过我都用来补妆的，还不错.\n",
            "情感预测结果: 负面\n",
            "输入语句: 此用户没有填写评论!\n",
            "情感预测结果: 负面\n",
            "输入语句: 此用户没有填写评论!\n",
            "情感预测结果: 负面\n",
            "输入语句: 宝贝不错哦\n",
            "情感预测结果: 负面\n",
            "输入语句: 宝贝，很好\n",
            "情感预测结果: 负面\n",
            "输入语句: 超级美丽了\n",
            "情感预测结果: 正面\n",
            "输入语句: 颜色很正略微有点粘杯\n",
            "情感预测结果: 负面\n",
            "输入语句: 送给女朋友的，希望她会喜欢\n",
            "情感预测结果: 负面\n",
            "输入语句: 包装很好显色度很高很饱和颜色特别好看\n",
            "情感预测结果: 负面\n",
            "输入语句: 喜欢喜欢超喜欢，味道说不出的好闻，自己都要被迷醉了！买的值了\n",
            "情感预测结果: 负面\n",
            "输入语句: 我家小狗子觉得还可以\n",
            "情感预测结果: 正面\n",
            "输入语句: 快递很快，这个颜色不是特别的艳丽，我还是喜欢很红很红的，哈哈\n",
            "情感预测结果: 正面\n",
            "输入语句: 此用户没有填写评论!\n",
            "情感预测结果: 负面\n",
            "输入语句: 第一次买了这么贵的气垫，但是个人觉得物有所值，我皮肤属于早上白，到下午就暗沉的那种，昨天四点多上妆，到晚上十点半左右，只有鼻子那里有点暗沉，其他都在线，不得不说一下赠送的明彩粉底液，超级好用啊，一点点就可以推全脸，而且不假白，很帖合我的皮肤，感觉皮肤超好，很细腻。对了，还有香水，很好闻，虽然不是我想要的黑鸦片，但是反转巴黎也很不错哦！下一步有点打算入手明彩粉底液啦！哈哈，被杨树林圈粉啦！\n",
            "情感预测结果: 负面\n",
            "输入语句: 好看，真不错，适合冬天\n",
            "情感预测结果: 负面\n",
            "输入语句: 不错女朋友很喜欢那我就很喜欢\n",
            "情感预测结果: 负面\n",
            "输入语句: 别人喜欢就好收下我就开心！ 做个好人挺好\n",
            "情感预测结果: 负面\n",
            "输入语句: 非常好用很喜欢😘\n",
            "情感预测结果: 负面\n",
            "输入语句: 包装高端大气上档次，用起来也非常非常好，\n",
            "情感预测结果: 负面\n",
            "输入语句: 还是觉得杨树林好用，从第一只开始就一发不可收拾，再多都觉得不够😄\n",
            "情感预测结果: 负面\n",
            "输入语句: 包装没话说，但是口红……一般般有点小失望\n",
            "情感预测结果: 负面\n",
            "输入语句: 非常好，痘印坑洼基本都能遮掉一天上班8小时，油性皮肤轻微出油 ysl 从未让我失望过\n",
            "情感预测结果: 负面\n",
            "输入语句: 前女友说很好看\n",
            "情感预测结果: 正面\n",
            "输入语句: 挺好的，还没用，感觉很不错，非常好，包装很好，快递给力，服务也很赞，推荐购买。\n",
            "情感预测结果: 负面\n",
            "输入语句: 雾面高光效果不错\n",
            "情感预测结果: 负面\n",
            "输入语句: 好看好看，包装也好看😂😂😂\n",
            "情感预测结果: 负面\n",
            "输入语句: 我买这款粉底液还是不错的，用起来很轻薄，自然，我是平时上班就涂上淡淡的一层，我喜欢自然，物流倒是很快，我在想，应该是正品，毕竟是官网嘛😊，我选择这款是客服为我推荐的，还挺合适我。不管好的不好的，用了再说吧，时间能证明一切，我到时候去专柜对比一下如果是正品我再要追评。希望没有让我失望吧\n",
            "情感预测结果: 负面\n",
            "输入语句: 男票喜欢，啥都好\n",
            "情感预测结果: 正面\n",
            "输入语句: 非常棒。看着就很高大上。喜欢。\n",
            "情感预测结果: 负面\n",
            "输入语句: 暂时不知道\n",
            "情感预测结果: 正面\n",
            "输入语句: 什么不脱色，骗人的，还这么贵\n",
            "情感预测结果: 正面\n",
            "输入语句: 货物收到了，很不错，与介绍相符合，快递也很快，很满意\n",
            "情感预测结果: 负面\n",
            "输入语句: 挺好的，滋润！\n",
            "情感预测结果: 负面\n",
            "输入语句: 东西很好，女朋友很喜欢，满分\n",
            "情感预测结果: 负面\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB5d9NEzEf8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}